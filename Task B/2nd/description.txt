In our second submission there is a csv file, called task_b.csv, containing our predictions on the test data made by using an LSTM network model. For this task we made use of some additional libraries which are tensorflow and sklearn. As configurations for our model, we went with a batch size of 30, a learning rate of 0.003, a dropout ratio 0.95, and an embedding size of 100. We preprocessed the data by lowercasing, removing hashtags and symbols that are not relevant for our classification system (e.g. digits, @, ", ' etc.). The weights for each of the two categories (UNT) and (TIN) were counted with respect to the number of representative tweets. We used an additional manually created dataset with the names of potential insult victims, which comprised top twitter accounts, ethic slurs and pronouns (e.g. “you”, “he” etc.)